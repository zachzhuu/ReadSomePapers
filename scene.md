# Scene Understanding Paper List

## Query

1. 25/04/19, Locate3D:Real-World Object Localization via Self-Supervised Learning in 3D, [[arxiv]](https://arxiv.org/pdf/2504.14151). Meta FAIR.
2. 25/04/27, OpenFusion++: An Open-vocabulary Real-time Scene Understanding System, [[arxiv]](https://arxiv.org/pdf/2504.19266)
3. 25/04/28, Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding, [[page]](https://mpec-3d.github.io/). CVPR25.

## Articulation

1. 25/04/22, DRAWER:DigitalReconstruction and Articulation With Environment Realism, [[page]](https://xiahongchi.github.io/DRAWER/).

## Navigation

1. 25/04/23, Graph2Nav: 3D **Object-Relation Graph** Generation to Robot Navigation, [[arxiv]](https://arxiv.org/pdf/2504.16782).
2. 25/02/06, A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks, [[page]](https://pku-epic.github.io/Uni-NaVid/). RSS25.
3. 25/05/16, Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation, [[arxiv]](https://arxiv.org/pdf/2505.11383).
4. 23/12/06, VLFM: Vision-Language **Frontier Maps** for Zero-Shot Semantic Navigation, [[page]](https://naoki.io/portfolio/vlfm)
5. 25/03/18, UniGoal: Towards Universal Zero-shot Goal-oriented Navigation, [[GitHub]](https://github.com/bagh2178/UniGoal?tab=readme-ov-file).
6. 24/04/09, GOAT-Bench: A Benchmark for Multi-modal Lifelong Navigation, [[page]](https://mukulkhanna.github.io/goat-bench/).
7. 25/02/17, NaVILA: Legged Robot Vision-Language-Action Model for Navigation, [[page]](https://navila-bot.github.io/). RSS25.
8. 25/04/11, Navigation World Models, [[page]](https://www.amirbar.net/nwm/). LeCun. CVPR25.
9. 25/04/22, ApexNav: An Adaptive Exploration Strategy for Zero-Shot Object Navigation with Target-centric Semantic Fusion, [[page]](https://robotics-star.com/ApexNav/). HKUST(GZ)
10. 25/05/15, NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance, [[page]](https://wzcai99.github.io/navigation-diffusion-policy.github.io/).

## Simulation

1. 25/04/26, Demonstrating DVS: Dynamic Virtual-Real Simulation Platform for Mobile Robotic Tasks, [[arxiv]](https://arxiv.org/abs/2504.18944). Relevant but messy paper.

## Dataset

1. 25/05/15, TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation, [[page]](https://tartanair.org/tartanground/). Diverse scenes.
2. 25/05/05, METASCENES: Towards Automated Replica Creation for Real-world 3D Scans, [[page]](https://meta-scenes.github.io/). Siyuan Huang's lab.

## Representation

1. 25/04/20, RoboOcc: Enhancing the Geometric and Semantic Scene Understanding for Robots, [[arxiv]](https://arxiv.org/pdf/2504.14604).

## Spatial Reasoning

1. 25/05/17, Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning, [[arxiv]](https://arxiv.org/pdf/2505.11907). HKUST(GZ).
2. 25/05/18, LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding, [[arxiv]](https://arxiv.org/pdf/2505.12253).
3. 25/05/18, Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring Expressions for Visual Understanding, [[arxiv]](https://arxiv.org/pdf/2505.12194).
4. 25/05/10, STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation, [[page]](https://zwandering.github.io/STRIVE.github.io/).

## Gen

1. 25/03/02, EgoSim: Egocentric Exploration in Virtual Worlds with Multi-modal Conditioning, [[page]](https://egosim.github.io/EgoSim/). ICLR25.
2. 25/05/05, Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation, [[page]](https://research.nvidia.com/labs/dir/scenethesis/). Nvidia.
3. 25/05/07, Steerable Scene Generation with Post Training and Inference-Time Search, [[page]](https://steerable-scene-generation.github.io/).
