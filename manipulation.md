# üìÅ Robotic Manipulation Paper List

## Vision Language Action Models

- February 2025 ‚Äî *Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success*  
  [Github](https://github.com/moojink/openvla-oft?tab=readme-ov-file). Stanford. Comparison with [œÄ‚ÇÄ](https://www.physicalintelligence.company/blog/pi0) and [RDT-1B](https://github.com/thu-ml/RoboticsDiffusionTransformer) included.
  
- May 2025 ‚Äî *3D-CAVLA: Leveraging Depth and 3D Context to Generalize Vision‚ÄìLanguage Action Models for Unseen Tasks*  
  [page](https://3d-cavla.github.io/). Discussed depth in VLA model.

- May 2025 ‚Äî *GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data*  
  [arxiv](https://arxiv.org/pdf/2505.03233)

- April 2025 ‚Äî *œÄ0.5: a Vision-Language-Action Model with Open-World Generalization*  
  [page](https://www.pi.website/blog/pi05). Physical Intelligence.

- May 2025 ‚Äî *OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning*  
  [page](https://one-two-vla.github.io/)

## Augmentation

- May 2025 ‚Äî *Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies Towards Visual Robustness*  
  [page](https://augmented-reality-for-robots.github.io/). A rough paper, but discussed a true problem (**visual robustness**).

- April 2025 ‚Äî *Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation*  
  [page](https://yangsizhe.github.io/robosplat/). RSS25

## Reconstruction

- April 2025 ‚Äî *Object Reconstruction Under Occlusion by Fusing Vision and Contact-Rich Physics*  
  [page](https://vysics-vision-and-physics.github.io/). GRASP Lab. RSS25

## Representation

- May 2025 ‚Äî *EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation*  
  [arxiv](https://arxiv.org/pdf/2505.10105). Submitted to NeurIPS25

## Dexterous from Human/Ego

- May 2025 ‚Äî *DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies*  
  [page](https://dexwild.github.io/). Why using Robot Data only doesn't work. CMU RI. RSS25

- May 2025 ‚Äî *EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video*  
  [arxiv](https://arxiv.org/pdf/2505.11709). Apple

- May 2025 ‚Äî *Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions*  
  [page](https://web2grasp.github.io/)

## Diffusion Policy

- March 2025 ‚Äî *ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy*  
  [page](https://et-seed.github.io/). Spatial generalization. ICLR25

## Scaling Robot Learning

- May 2025 ‚Äî *Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware*  
  [page](https://real2render2real.com/). Berkeley

- May 2025 ‚Äî *X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real*  
  [page](https://portal-cornell.github.io/X-Sim/). Cornell

- May 2025 ‚Äî *DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories*  
  [page](https://research.nvidia.com/labs/gear/dreamgen/). Nvidia

- April 2025 ‚Äî *RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning*  
  [page](https://roboverseorg.github.io/)

- March 2025 ‚Äî *What Matters in Learning from Large-Scale Datasets for Robot Manipulation*  
  [page](https://robo-mimiclabs.github.io/pages/study.html). ICLR25

## World Model

- May 2018 ‚Äî *World Models*  
  [page](https://worldmodels.github.io/). NIPS18

- May 2025 ‚Äî *ENERVERSE-AC: Envisioning Embodied Environments with Action Condition*  
  [page](https://annaj2178.github.io/EnerverseAC.github.io/). Agibot

- April 2025 ‚Äî *Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets*  
  [page](https://weirdlabuw.github.io/uwm/). RSS25

- March 2025 ‚Äî *Learning View-invariant World Models for Visual Robotic Manipulation*  
  [GitHub](https://github.com/lafmdp/ReViWo). ICLR25

- May 2025 ‚Äî *LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation*  
  [arxiv](https://arxiv.org/pdf/2505.11528)

- May 2025 ‚Äî *EWMBENCH: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models*  
  [GitHub](https://github.com/AgibotTech/EWMBench)

- April 2025 ‚Äî *TesserAct: Learning 4D Embodied World Models*  
  [page](https://tesseractworld.github.io/)

- May 2025 ‚Äî *Learning 3D Persistent Embodied World Models*  
  [arxiv](https://arxiv.org/pdf/2505.05495)

- May 2025 ‚Äî *Vid2World: Crafting Video Diffusion Models to Interactive World Models*  
  [page](https://knightnemo.github.io/vid2world/)

- May 2025 ‚Äî *PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation*  
  [page](https://pinwm.github.io/). RSS25

- May 2025 ‚Äî *Occupancy World Model for Robots*  
  [arxiv](https://arxiv.org/pdf/2505.05512)

## Humanoid from Human

- May 2025 ‚Äî *Visual Imitation Enables Contextual Humanoid Control*  
  [page](https://www.videomimic.net/). Berkeley

## Vision

- May 2025 ‚Äî *Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation*  
  [page](https://aalmuzairee.github.io/mad/)
