# Robotic Manipulation Paper List

## Vision Language Action Models

1. 25/02/27, Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success, [[Github]](https://github.com/moojink/openvla-oft?tab=readme-ov-file). Stanford. Comparison with [$\pi_0$](https://www.physicalintelligence.company/blog/pi0) and [RDT-1B](https://github.com/thu-ml/RoboticsDiffusionTransformer) included.
2. 25/05/09, 3D-CAVLA: Leveraging Depth and 3D Context to Generalize Vision–Language Action Models for Unseen Tasks, [[page]](https://3d-cavla.github.io/). Discussed depth in VLA model.
3. 25/05/06, GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data, [[arxiv]](https://arxiv.org/pdf/2505.03233).
4. 25/04/22, π0.5: a Vision-Language-Action Model with Open-World Generalization, [[page]](https://www.pi.website/blog/pi05). Physical Intelligence.
5. 25/05/17, OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning, [[page]](https://one-two-vla.github.io/).

## Augmentation

1. 25/05/13, Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies Towards Visual Robustness, [[page]](https://augmented-reality-for-robots.github.io/). A rough paper, but discussed a true problem (**visual robustness**).
2. 25/04/17, Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation, [[page]](https://yangsizhe.github.io/robosplat/). RSS25.

## Reconstruction

1. 25/04/25, Object Reconstruction Under Occlusion by Fusing Vision and **Contact-Rich Physics**, [[page]](https://vysics-vision-and-physics.github.io/). GRASP Lab. RSS25.

## Representation

1. 25/05/15, EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation, [[arxiv]](https://arxiv.org/pdf/2505.10105). Submitted to NeurIPS25.

## Dexterous from Human/Ego

1. 25/05/12, DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies, [[page]](https://dexwild.github.io/). Why using Robot Data only doesn't work. CMU RI. RSS25.
2. 25/05/16, EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video, [[arxiv]](https://arxiv.org/pdf/2505.11709). Apple.
3. 25/05/13, Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions, [[page]](https://web2grasp.github.io/).

## Diffusion Policy

1. 25/03/02, ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy, [[page]](https://et-seed.github.io/). Spatial generalization. ICLR25.

## Scaling Robot Learning

1. 25/05/14, Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware, [[page]](https://real2render2real.com/). Berkeley.
2. 25/05/15, X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real, [[page]](https://portal-cornell.github.io/X-Sim/). Cornell.
3. 25/05/20, DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories, [[page]](https://research.nvidia.com/labs/gear/dreamgen/). Nvidia.
4. 25/04/26, RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning, [[page]](https://roboverseorg.github.io/).
5. 25/03/02, What Matters in Learning from Large-Scale Datasets for Robot Manipulation, [[page]](https://robo-mimiclabs.github.io/pages/study.html). ICLR25.

## World Model

1. 18/05/09, World Models, [[page]](https://worldmodels.github.io/). NIPS18.
2. 25/05/14, ENERVERSE-AC: Envisioning Embodied Environments with Action Condition, [[page]](https://annaj2178.github.io/EnerverseAC.github.io/). Agibot.
3. 25/04/03, Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets, [[page]](https://weirdlabuw.github.io/uwm/). RSS25.
4. 25/03/02, Learning View-invariant World Models for Visual Robotic Manipulation, [[GitHub]](https://github.com/lafmdp/ReViWo). ICLR25.
5. 25/05/13, LaDi-WM: ALatent Diffusion-based World Model for Predictive Manipulation, [[arxiv]](https://arxiv.org/pdf/2505.11528).
6. 25/05/18, EWMBENCH: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models, [[GitHub]](https://github.com/AgibotTech/EWMBench).
7. 25/04/29, TesserAct: Learning 4D Embodied World Models, [[page]](https://tesseractworld.github.io/).
8. 25/05/05, Learning 3D Persistent Embodied World Models, [[arxiv]](https://arxiv.org/pdf/2505.05495).
9. 25/05/20, Vid2World: Crafting Video Diffusion Models to Interactive World Models, [[page]](https://knightnemo.github.io/vid2world/).
10. 25/05/03, PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation, [[page]](https://pinwm.github.io/). RSS25.
11. 25/05/07, Occupancy World Model for Robots, [[arxiv]](https://arxiv.org/pdf/2505.05512).

## Humanoid from Human

1. 25/05/13, Visual Imitation Enables Contextual Humanoid Control, [[page]](https://www.videomimic.net/). Berkeley.

## Vision

1. 25/05/07, Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation, [[page]](https://aalmuzairee.github.io/mad/).
